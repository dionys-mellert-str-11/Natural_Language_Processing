{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98c5c304-c58c-4f97-947c-49a98a187af3",
    "_uuid": "f4032b042154661728b4b2d7851a21530a66e621"
   },
   "source": [
    "# Basic example of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "d19d5e90-a77a-48a4-b789-e78753e369cd",
    "_uuid": "0eb16b0a1e4b74ccad01f47370bf16e6ba5d0baa"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization is a step in feature extraction. Why we do that? because to get some useful features out of the text. It means, we are converting from text to numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "ba40f73f-7095-4df4-af77-a8bdc5294fe6",
    "_uuid": "3263da18041071f7b324e3c9a88f1feae5720be0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "27152481-d49f-4dcf-ba59-a1b666744279",
    "_uuid": "99d1a5edc81162dd53e3cf404e613ae92d5a657d"
   },
   "source": [
    "Let’s use it to tokenize and count the word occurrences of text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b069d6f1-3a7d-4244-8554-b06a85d25489",
    "_uuid": "9ee0592c12ef4bafbd886adc208ab2c07d6f1380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['Hi my name is Erwin.','I love traveling.','Erwin loves eating delicious food.']\n",
    "my_doc = vect.fit_transform(corpus)\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56b04008-af4a-4693-ac87-9082013513a6",
    "_uuid": "9be3b64dbd7d47f7180b50567b42af6628589b8e"
   },
   "source": [
    "From the information above, the dimension of my_doc is 3x11. <br>\n",
    "It means, 3 rows and 11 columns as there are 3 documents and 11 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "7c1a646b-a698-49ac-ba03-776daf18b68f",
    "_uuid": "aa0e66b552c7f11339caa93baa36caf5084714e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['delicious', 'eating', 'erwin', 'food', 'hi', 'is', 'love',\n",
       "       'loves', 'my', 'name', 'traveling'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before, there are 11 unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5cbcfa14-baee-4a46-af66-011bc24c6a01",
    "_uuid": "bab67cdd065aa5dc7b28da8cee62b823140d771d"
   },
   "source": [
    "Each term found by the analyzer during the analysis is assigned a unique integer index corresponding to a column in the resulting matrix found in \"vect.get_feature_names_out()\". <br>\n",
    "<br>\n",
    "This interpretation of the columns can be retrieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "c25ac4c3-2283-4822-b01c-98634f216e8e",
    "_uuid": "5a3fa876b5b4770d411c5870e982d384fd763a62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_doc.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4be526e6-c7b7-45e0-9206-03916234fb68",
    "_uuid": "faffe49878430e8b73ce2b58b70976d3a9410ddf"
   },
   "source": [
    "If there are new words that are not included in training corpus, they will be ignored in the transform method. <br>\n",
    "For example: \"Hallo, how old are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "022e678a-bd1a-45ce-abbc-6c1ff7c2d945",
    "_uuid": "a229b172df59e679eedb8f91b4176cc8f7bbba94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['Hallo, how old are you?']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the array is filled with zero since there is no matching words in training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7606035a-e08d-42b3-adfe-690c3b72f248",
    "_uuid": "ab3c29d7c170928c5f2bb2b6d7d7949358fd5cc6"
   },
   "source": [
    "## Normalization and stemming <br>\n",
    "the word \"love\" and \"loves\" has same meaning. That's why we should treat them the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "cf31de02-63be-459c-b23a-dd316c1e5fa8",
    "_uuid": "4acdb60ec8ab8761e1a2f2f9879f1b0c8c003533"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delici',\n",
       " 'eat',\n",
       " 'erwin',\n",
       " 'food',\n",
       " 'hi',\n",
       " 'is',\n",
       " 'love',\n",
       " 'love',\n",
       " 'my',\n",
       " 'name',\n",
       " 'travel']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in vect.get_feature_names_out()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95f4f41d-4f0d-4ddc-a013-50f17a825550",
    "_uuid": "c5cfdc86f340649b0f1515c43fe41445e0a53ea5"
   },
   "source": [
    "Now the word \"loves\" has become \"love\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "331447f9-bf8a-4565-9dd0-3ca290eb80ee",
    "_uuid": "59ec85c98e287e17031689b12155a736097f549c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delici', 'food', 'travel', 'name', 'is', 'love', 'erwin', 'eat', 'my', 'hi']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([porter.stem(t) for t in vect.get_feature_names_out()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, now we have only 10 unique words (previously was 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "46c00b36-7a39-4836-87c1-c6c12e0d4112",
    "_uuid": "cf1dbd710652ff529bfc380667bf3fdcd371bea3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delici', 'food', 'travel', 'name', 'is', 'love', 'erwin', 'eat', 'my', 'hi']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "[WNlemma.lemmatize(t) for t in list(set([porter.stem(t) for t in vect.get_feature_names_out()]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8102f0f-ce68-4584-a578-3a9f24a47678",
    "_uuid": "0dcfd364a150043cd6890471fc40cd6c99573949"
   },
   "source": [
    "## Lemmatization <br>\n",
    "\n",
    "similar case to stemming is called lemmatizing. <br>\n",
    "The main difference between those two as you saw earlier example:\n",
    "stemming can often create non-existent words, whereas lemmas are actual words.<br>\n",
    "\n",
    "Sometimes, the meaning of word you generate with, is not found in  dictionary, However, you can look up a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "ecad571a-f85c-4db4-a115-f2f35c31c45d",
    "_uuid": "9622010ce3f85837073aa66d39ffd9c59bf682b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "doggy\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "snake\n",
      "good\n",
      "best\n",
      "good\n",
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"dogs\"))\n",
    "print(lemmatizer.lemmatize(\"doggies\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"snakes\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"good\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))\n",
    "print(lemmatizer.lemmatize(\"running\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b36c6c6dc68256f8fe6689e745f2c9ef656ed9d"
   },
   "source": [
    "Part of speech tagging: is a tool which tags a particular sentence or words in a paragraph by looking at the context of the sentence/words inside the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "a9e8b48a33d6abcbb17f9735982790236e1c34fb"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "sentence = \"Python is the best programming language to learn Data Science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "a14c7d2110d89f542874063c393e9db1f736be73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('best', 'JJS'),\n",
       " ('programming', 'NN'),\n",
       " ('language', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('learn', 'VB'),\n",
       " ('Data', 'NNP'),\n",
       " ('Science', 'NNP')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_token = word_tokenize(sentence)\n",
    "pos_tag(sen_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2f8517763ca70425fb005398a25022416a5388a"
   },
   "source": [
    "As can be seen, the words are tagged by various parts of speech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc97285fb5c797d0041a31862eca506bf475c9de"
   },
   "source": [
    "* ![](https://cdn-images-1.medium.com/max/800/0*V635bzjWK2n1jBsd.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
